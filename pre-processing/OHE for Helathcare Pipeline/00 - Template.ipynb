{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE For HealthCare Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn import metrics\n",
    "# from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the heart.csv file into a dataframe called df, and view a 2% random sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info on df columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into Xtrain, Xtest, ytrain, ytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into Xtrain, Xtest, ytrain, ytest - with 20% in test, and random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code to deal with SetttingWithCopyWarning, and ensure we are working with a copy of the data and not a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic stats on Xtrain (such as count, mean, std, etc)\n",
    "\n",
    "# Xtrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any NaNs in any of the columns in Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two lists called numeric_features (with age, trestbps, chol, thalach, oldpeak), and \n",
    "# categorical_features (with sex, cp, fbs, restecg, exang, slope, ca, thal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scale all of the numeric features in Xtrain, include transformed numeric features in Xtrain, \n",
    "# and drop original numeric features in Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode all of the categorical features in Xtrain, include transformed categorical features in Xtrain,\n",
    "# and drop original categorical features in Xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Logistic Regression Model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Logistic Regression Model to training data with random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scale all of the numeric features in Xtest, include transformed numeric features in Xtest, \n",
    "# and drop original numeric features in Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode all of the categorical features in Xtest, include transformed categorical features in Xtest, \n",
    "# and drop original categorical features in Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Evaluate Logisitic Regression Model on Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Evaluate Logisitic Regression Model on Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - (a) Read in heart.csv in to a dataframe called df; \n",
    "# - (b) Split the data into Xtrain, Xtest, ytrain, ytest - with 20% in test, and random_state=1; \n",
    "# - (c) Create two lists called numeric_features (with age, trestbps, chol, thalach, oldpeak), \n",
    "#      and categorical_features (with sex, cp, fbs, restecg, exang, slope, ca, thal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to deal with SetttingWithCopyWarning, and ensure we are working with a copy of the data and not a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline called \"numeric_transformer\" with a StandardScaler step called \"ss\" \n",
    "# (use the same parameters that you used in Part A above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline called \"categorical_transformer\" with a OneHotEncoder step called \"ohe\" \n",
    "# (use the same parameters that you used in Part A above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column transformer called \"preprocessor\" with two transformers: \n",
    "# (a) the first transformer called \"num\" which uses the numeric_transformer you defined above on the numeric_features; and \n",
    "# (b) the second transformer called \"cat\" which uses the categorical_transformer you defined above on the categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline called \"clf\" with two steps: \n",
    "# (a) the first step called \"pp\" which invokes the preprocessor you defined above; and \n",
    "# (b) the second step called \"lr\" which involkes a logisitc regression model \n",
    "# (use the same parameters that you used in Part A above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the clf pipeline to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Evaluate clf pipeline on Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred = clf.predict(Xtest)\n",
    "# from sklearn import metrics\n",
    "# print (metrics.accuracy_score(ytest, ypred))\n",
    "# print (metrics.confusion_matrix(ytest, ypred))\n",
    "# print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
