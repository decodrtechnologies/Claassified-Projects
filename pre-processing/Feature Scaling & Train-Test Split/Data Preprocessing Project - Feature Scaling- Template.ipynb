{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Project – Feature Scaling\n",
    "\n",
    "\n",
    "In this project, we had  discussed various data preprocessing techniques related to feature scaling.\n",
    "\n",
    "\n",
    "The contents of this project are divided as follows:-\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "1.\tIntroduction\n",
    "\n",
    "2.\tRescaling data with MinMaxScaler\n",
    "\n",
    "3.\tStandardising data with StandardScaler\n",
    "\n",
    "4.  Rescaling data with MaxAbsScaler\n",
    "\n",
    "5.  Rescaling data with RobustScaler\n",
    "\n",
    "6.\tNormalizing data with Normalizer\n",
    "\n",
    "7.\tBinarizing data with Binarizer\n",
    "\n",
    "8.\tMean removal with scale\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "One of the most important data preprocessing step, we need to apply to our data is feature scaling. When we encounter any real world data set, the independent or feature variables may be mapped onto different scales.  **Feature scaling** refers to procedures used to standardize these independent or feature variables so that they are mapped onto same scales.\n",
    "\n",
    "\n",
    "Most of the ML algorithms perform well when the feature variables are mapped onto the same scale. They don’t perform well when features are mapped onto different scales. For example, in stochastic gradient descent, feature scaling can improve the convergence speed of the algorithm. In support vector machines, it can reduce the time to find support vectors. \n",
    "\n",
    "\n",
    "But, there are few exceptions as well. Decision trees and random forests are two of the algorithms where we don’t need to worry about feature scaling. These algorithms are scale invariant. Similarly, Naive Bayes and Linear Discriminant Analysis are not affected by feature scaling. In Short, any Algorithm which is not distance based is not affected by feature scaling.\n",
    "\n",
    "So, let’s start our discussion of various techniques associated with feature scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the warnings\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rescaling data with MinMaxScaler\n",
    "\n",
    "\n",
    "This technique of rescaling is also called **min-max scaling** or **min-max normalization**. **Normalization** refers to the rescaling of the features to a range of [0, 1], which is a special case of min-max scaling. So, in this technique, values are shifted and rescaled so that they end up ranging from zero to one. We do this by subtracting the minimum value (xmin ) and dividing by the maximum value (xmax ) minus the minimum value (xmin ). \n",
    "\n",
    "\n",
    "Mathematically, the new value x(i)norm of a sample x(i) can be calculated as follows:-\n",
    "\n",
    " \n",
    "\t\t     x(i)norm  =  (xi-  xmin )/(xmax-  xmin )\n",
    "             \n",
    "\t\n",
    "Here, x(i) is a particular sample value. xmax and xmin is the maximum and minimum feature value in a column.\n",
    "\n",
    "\t\n",
    "Scikit-Learn provides a transformer called **MinMaxScaler** for this task. It has a feature range parameter to adjust the range of values. This estimator fit and transform each feature variable individually such that it is in the given range (between zero and one) on the training set. \n",
    "\n",
    "**MinMaxScaler** works well for the cases when the distribution is not normal or when the standard deviation is very small. But, it has one drawback. It is sensitive to outliers\n",
    "\n",
    "\n",
    "The syntax for implementing min-max scaling procedure in Scikit-Learn is given as follows:- \n",
    "\n",
    "\n",
    "`from sklearn.preprocessing import MinMaxScaler`\n",
    "\n",
    "`ms = MinMaxScaler()`\n",
    "\n",
    "`X_train_ms = ms.fit_transform(X_train)`\n",
    "\n",
    "`X_test_ms = ms.transform(X_test)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset to demonstrate minmaxscaling\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    # positive skew\n",
    "    'x1': np.random.chisquare(10, 1000),\n",
    "    # negative skew \n",
    "    'x2': np.random.beta(10, 2, 1000) * 40,\n",
    "    # no skew\n",
    "    'x3': np.random.normal(50, 3, 1000)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMaxScaler to apply minmaxscaling\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and visualize the results\n",
    "\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standardising data with StandardScaler\n",
    "\n",
    "\n",
    "There is another practical approach for feature scaling which might be more useful in certain circumstances. It is called **standardization**. It can be more useful for many machine learning algorithms, especially for optimization algorithms such as gradient descent.\n",
    "\n",
    "\n",
    "In **standardization**, first we determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values of each feature by its standard deviation. So, in standardization, we center the feature columns at mean 0 with standard deviation 1 so that the feature columns takes the form of a normal distribution, which makes it easier to learn the weights.\n",
    "\n",
    "\n",
    "Scikit-Learn provides a transformer called **StandardScaler** for standardization. The **StandardScaler** transformer assumes that the data is normally distributed within each feature and it will scale them such that the distribution is centered around 0 and and have a standard deviation of 1.\n",
    "\n",
    "\n",
    "Mathematically, **standardization** can be expressed by the following equation: \n",
    "\n",
    "\n",
    "\t\tx(i)std =  ( x(i)- μx)/(σx )\n",
    "\n",
    "\n",
    "Here, x(i) is a particular sample value and x(i)std is its standard deviation , μx is the sample mean of a particular feature column and σx is the corresponding standard deviation.\n",
    "\n",
    "\n",
    "Min-max scaling scales the data to a limited range of values. Unlike min-max scaling, standardization does not bound values to a specific range. So, standardization is much less affected by outliers. Standardization maintains useful information about outliers and is much less affected by them. It makes the algorithm less sensitive to outliers in contrast to min-max scaling. \n",
    "\n",
    "\n",
    "The syntax to implement standardization is quite similar to min-max scaling given as follows :-\n",
    "\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler`\n",
    "\n",
    "`ss = StandardScaler()`\n",
    "\n",
    "`X_train_ss = ss.fit_transform(X_train)`\n",
    "\n",
    "`X_test_ss = ss.transform(X_test)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset to demonstrate standardization\n",
    "\n",
    "np.random.seed(1)\n",
    "df2 = pd.DataFrame({\n",
    "            'x1':np.random.normal(0, 10, 10000),\n",
    "            'x2':np.random.normal(10, 20, 10000),\n",
    "            'x3':np.random.normal(-10, 10, 10000)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to apply Standardisation\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and visualize the results\n",
    "\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rescaling data with MaxAbsScaler\n",
    "\n",
    "\n",
    "\n",
    "In this feature rescaling task, we rescale each feature by its maximum absolute value. So, the maximum absolute value of each feature in the training set will be 1.0. It does not affect the data and hence there is no effect on sparsity.\n",
    "\n",
    "Scikit-Learn provides **MaxAbsScaler** transformer for this task.\n",
    "\n",
    "The syntax for implementing max-abs scaling procedure in Scikit-Learn is given as follows:- \n",
    "\n",
    "\n",
    "`from sklearn.preprocessing import MaxAbsScaler`\n",
    "\n",
    "`mabs = MaxAbsScaler()`\n",
    "\n",
    "`X_train_mabs = mabs.fit_transform(X_train)`\n",
    "\n",
    "`X_test_mabs = mabs.transform(X_test)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset to demonstrate maxabsscaling\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    # positive skew\n",
    "    'x1': np.random.chisquare(10, 1000),\n",
    "    # negative skew \n",
    "    'x2': np.random.beta(10, 2, 1000) * 40,\n",
    "    # no skew\n",
    "    'x3': np.random.normal(50, 3, 1000)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MaxAbsScaler to apply maxabsscaling\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and visualize the results\n",
    "\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rescaling using RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "**StandardScaler** can often give misleading results when the data contain outliers.  Outliers can often influence the sample mean and variance and hence give misleading results. In such cases, it is better to use a scalar that is robust against outliers. Scikit-Learn provides a transformer called **RobustScaler** for this purpose.\n",
    "\n",
    "\n",
    "The **RobustScaler** is very similar to **MinMaxScaler**. The difference lies in the parameters used for scaling. \n",
    "While **MinMaxScaler** uses minimum and maximum values for rescaling, **RobustScaler** uses interquartile(IQR) range \n",
    "for the same.\n",
    "\n",
    "\n",
    "\n",
    "Mathematically, the new value x(i)norm of a sample x(i) can be calculated as follows:-\n",
    "\n",
    " \n",
    "\t\t     x(i)  =  (xi-  Q1(x) )/(Q3(x) - Q1(x))\n",
    "             \n",
    "\t\n",
    "Here, x(i) is the scaled value, xi is a particular sample value, Q1(x) and Q3(x) are the 1st quartile (25th quantile) and \n",
    "3rd quartile (75th quantile) respectively. So, Q3(x) - Q1(x) is the difference between 3rd quartile (75th quantile) and \n",
    "1st quartile (25th quantile) respectively. It is called IQR (Interquartile Range).\n",
    "\n",
    "\t\n",
    "\n",
    "The syntax for implementing scaling using RobustScaler in Scikit-Learn is given as follows:- \n",
    "\n",
    "\n",
    "`from sklearn.preprocessing import RobustScaler`\n",
    "\n",
    "`rb = RobustScaler()`\n",
    "\n",
    "`X_train_rb = rb.fit_transform(X_train)`\n",
    "\n",
    "`X_test_rb = rb.transform(X_test)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset to demonstrate scaling using RobustScaler\n",
    "\n",
    "df4 = pd.DataFrame({\n",
    "    # Distribution with lower outliers\n",
    "    'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n",
    "    # Distribution with higher outliers\n",
    "    'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RobustScaler for scaling\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and visualize the results\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalizing data with Normalizer\n",
    "\n",
    "\n",
    "In this feature scaling task, we rescale each observation to a length of 1 (a unit norm). Scikit-Learn provides, the **Normalizer** class for this task. In this task, we scale the components of a feature vector such that the complete vector \n",
    "has length one. \n",
    "\n",
    "\n",
    "This usually means dividing each component by the Euclidean length (magnitude) of the vector.\n",
    "\n",
    "\n",
    "Mathematically, **normalization** can be expressed by the following equation: \n",
    "\n",
    "\n",
    "`x(i)norm =   x(i) / | x(i)|`\n",
    "\n",
    "\n",
    "where  x(i) is a particular sample value , x(i)norm is its normalized value and | x(i)| is the corresponding Euclidean length of the vector. \n",
    "\n",
    "\n",
    "The syntax for normalization is quite similar to standardization given as follows:-\n",
    "\n",
    "\n",
    "`from sklearn.preprocessing import Normalizer`\n",
    "\n",
    "`nm = Normalizer()`\n",
    "\n",
    "`X_train_nm = nm.fit_transform(X_train)`\n",
    "\n",
    "`X_test_nm = nm.transform(X_test)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset to demonstrate Normalization\n",
    "\n",
    "df5 = pd.DataFrame({\n",
    "    'x1': np.random.randint(-100, 100, 1000).astype(float),\n",
    "    'y1': np.random.randint(-80, 80, 1000).astype(float),\n",
    "    'z1': np.random.randint(-150, 150, 1000).astype(float),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Normalizer to implement Normalization\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and visualize the results\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Binarizing data with Binarizer\n",
    "\n",
    "\n",
    "In this feature scaling procedure, we binarize the data (set feature values equal to 0 or 1) according to a threshold. \n",
    "So, using a binary threshold, we transform our data by marking the values above it to 1 and those equal to or below it to 0. \n",
    "\n",
    "\n",
    "Scikit-Learn provides **Binarizer** class for this purpose. The syntax for binarizing the data follow the same rules as above and is given below:-\n",
    "\n",
    "\n",
    "`from sklearn.preprocessing import Binarizer`\n",
    "\n",
    "`binr = Binarizer()`\n",
    "\n",
    "`X_train_binr = binr.fit_transform(X_train)`\n",
    "\n",
    "`X_test_binr = binr.transform(X_test)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset to demonstrate binarization \n",
    "\n",
    "data1 = [[2, -2, 1],\n",
    "        [5, -5, 3],\n",
    "        [1, 0, -1]]\n",
    "\n",
    "df6 = pd.DataFrame(data1, columns = ['x1', 'x2', 'x3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Binarizer to apply binarization\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "\n",
    "## start  code\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mean removal with scale\n",
    "\n",
    "\n",
    "In this feature scaling task, we remove the mean from each feature to centre it on zero. Thus, we standardize a dataset \n",
    "along any axis. Scikit-Learn provides **scale** class for this purpose. \n",
    "\n",
    "\n",
    "The syntax for this purpose is given below:-\n",
    "\n",
    "\n",
    "`from sklearn.preprocessing import scale`\n",
    "\n",
    "`scl = scale()`\n",
    "\n",
    "`X_train_scl = scl.fit_transform(X_train)`\n",
    "\n",
    "`X_test_scl = scl.transform(X_test)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset to demonstrate standardization using scale\n",
    "\n",
    "data2 = [[5, -5, 1],\n",
    "        [2, -2, 3],\n",
    "        [1, 0, -1]]\n",
    "\n",
    "df7 = pd.DataFrame(data2, columns = ['x4', 'x5', 'x6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scale to apply standardization\n",
    "\n",
    "## start  code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "\n",
    "## start  code\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scaled data has zero mean\n",
    "\n",
    "(scaled_df7.mean(axis = 0).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scaled data has unit variance\n",
    "\n",
    "(scaled_df7.std(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your conclusions here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
