{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a0cba43a48031c985f674fe90e17bc89af95460e"
   },
   "source": [
    "# Q-Learning with ANN ü§ñ üéõ ‚òéÔ∏è \n",
    "> This is a demo of Q-learning for solving Reinforcement Learning (RL) problems. The challenge here is a frozen lake that is being solved via **Artificial Neural Network (ANN)** approach for Q-learning implementation. It serves as an example code for an [introduction story into RL on Medium](https://towardsdatascience.com/lite-intro-into-reinforcement-learning-857ca5c924d9).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tomtx/q-learning-demo/master/img/ann-wall-e.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "## 1) Environment üó∫\n",
    "It is a [frozen lake by Gym OpenAI](https://gym.openai.com/envs/FrozenLake-v0/), in which an agent tries to find a safe path across a grid of slippery ice and water tiles to get the ultimate reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "2383ca828a1eb7f2c0c9a3c77daf7290e5a7855b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    ".--------------------.\n",
    "| üêπ | ‚ùÑÔ∏è | ‚ùÑÔ∏è | ‚ùÑÔ∏è | \n",
    "|- - - - -  - - - - -|\n",
    "| ‚ùÑÔ∏è | üï≥ | ‚ùÑÔ∏è | üï≥ |\n",
    "|- - - - -  - - - - -|\n",
    "| ‚ùÑÔ∏è | ‚ùÑÔ∏è | ‚ùÑÔ∏è | üï≥ |\n",
    "|- - - - -  - - - - -|\n",
    "| üï≥ | ‚ùÑÔ∏è | ‚ùÑÔ∏è | ü•ú |\n",
    ".--------------------.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cfbece799b97db08ade004140525a8f32dc804b"
   },
   "source": [
    "The environment is depicted in the picture above as a 4x4 grid world with 16 possible states. There is a hamster that tries to munch on peanuts. Its **goal is to reach those peanuts**!\n",
    "* this little fellow can choose to go one of **4 directions (actions) on the ice at each grid cell (state)**, i.e. it can go either up, down, left, or right\n",
    "* it tries to **avoid going into water holes**, which results in sinking and therefore an unsuccessful attempt\n",
    "* however, it is not that easy since there is a chance that this little creature can **randomly slip on ice and go into different grid cells** than wanted\n",
    "* the **success or failure** for solving the frozen lake is defined by this hamster getting those peanuts or sinking into the water hole, respectively\n",
    "\n",
    "## 2) Algorithm üé∞\n",
    "It is a classic Q-learning algorithm that goes through iteration steps to experience the environment. After certain amount of episodes, it ultimately finds the best state-action pairs Q(s,a) for the algorithm, the so called \"Q-values\". The Q-values represent how good it is for an agent (üêπ) to be in state in terms of getting the long term reward (ü•ú). This algorithm has various implementations depending on which policy is chosen for taking actions or how Q-values are processed. The picture below shows an overview on the implementation of this model-free algorithm from the Temporal Difference learning (TD) method [Sutton & Barto 2017].\n",
    "\n",
    "![title](https://raw.githubusercontent.com/tomtx/q-learning-demo/master/img/algo-method.png)\n",
    "> _... let's see the implementation!_ üõ†\n",
    "\n",
    "### ANN ‚òéÔ∏è\n",
    "We will **create and train a simple ANN-like model** that will serve us as a universal function approximator resembling optimal Q-values for the Q-learning algorithm. The Q-values are represented here via a weights matrix. This matrix is initialized with some values at start and gets populated / trained with different values as the algorithm proceeds. It ultimately reaches some optimal values for an agent to choose (higher Q-values are better), once it has learned how to solve the environment. This means that Q-values are learned after a certain amount of episodes (training steps/iterations) & can be then used to play the game (solve the environment) better than without any prior training.\n",
    "\n",
    "This neural net is hopefully as simple as possible, since it is fed forward only into a single layer of output nodes and has a simple optimizer for training. It is loosely shown in the picture below and has the following properties.\n",
    "* **16 neurons in the input layer** are representing all possible states (s) -> a [1,16] shaped matrix\n",
    "* a **weights matrix** is representing Q-values, where its **4 columns** are representing possible actions (U for up, D for down, L for left, R for right) & its **16 rows** representing all states for this environment -> a [16,4] shaped matrix\n",
    "* **4 neurons in the output layer** are representing values for possible actions (a) in the particular state (s), i.e. Q-values for that state -> a [1,4] shaped matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "ea9b531ceb800b4daa8c4dad1818a47d25bf51ad"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  input        weights         output\n",
    " (states)     (Q-values)       (Q-values for actions in that particular state)  \n",
    "  \n",
    "             U   D   L   R\n",
    "           .---------------.\n",
    "0    o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "1    o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "2    o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "3    o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "4    o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "5    o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "6    o-   -| Q | Q | Q | Q |-   -o U\n",
    "           | - - - - - - - |\n",
    "7    o-   -| Q | Q | Q | Q |-   -o D\n",
    "           | - - - - - - - |\n",
    "8    o-   -| Q | Q | Q | Q |-   -o L\n",
    "           | - - - - - - - |\n",
    "9    o-   -| Q | Q | Q | Q |-   -o R\n",
    "           | - - - - - - - |\n",
    "10   o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "11   o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "12   o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "13   o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "14   o-   -| Q | Q | Q | Q |- \n",
    "           | - - - - - - - |\n",
    "15   o-   -| Q | Q | Q | Q |- \n",
    "           .---------------.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "628d0ec81d3f6fdbae92d03d901780ef8c16ad77"
   },
   "source": [
    "The **feed-forward process** for **obtaining Q-values for a given state** from this neural net works as following:\n",
    "* input layer receives an information about which state is being evaluated in the environment,\n",
    "* then a particular neuron of the input layer belonging to that particular state fires,\n",
    "* information travels to output layer while grabbing Q-values for that state from weights matrix,\n",
    "* and this results into output layer receiving Q-values for actions in that particular state.\n",
    "\n",
    "#### approximating üìê\n",
    "The Q-learning tells us through Q-values what reward is there for taking an action in a state of the environment. Our neural net will try to resemble Q-values by approximating them. It will be designed to arrive at some optimal Q-values by first using an 'update rule' that estimates new Q-values and then by using a loss function, which expresses an error brought on by such estimation. We will then use such error to update the Q-values through some optimization technique during training (shown in the next section). These procedures will be performed for the each training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "300ea362d6ff8083401a759746bc947f0667f61b"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tomtx/q-learning-demo/master/img/algo-update_rule.png\" alt=\"Drawing\" align=\"left\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d59fa88683a5769a058a66bfc95e35f5a79f315"
   },
   "source": [
    "The equation above shows our 'update rule' that is based on Bellman equations. It is used in the TD method, where the right-hand side of equation is usually called a TD target. It describes the update of the new Q-value. In this equation, such value is obtained through scaling the maximum Q-value for the new state (s π) through discount factor (…£) and then adding a reward (R) to it. Also, the difference between both sides of this equation is known as TD error. This is the error which we are trying to capture with our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8b6bc1882bd416fc9ac734f0927cb43ccc13a80"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tomtx/q-learning-demo/master/img/algo-loss_l2.png\" alt=\"Drawing\" align=\"left\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "993b8323cc856897f70b6b18ec3cb903bb50a5b5"
   },
   "source": [
    "We will also use a simple L2 Euclidean norm to represent an error brought on by this approximation of Q-values (the introduced update rule). The L2-norm assigns a possitive length to the vector describing such error. As shown in the equation above, this vector norm is implemented same as the least squares method in regression analysis which minimizes the sum of the square of the differences between dependent (y) and independent (x) variables at each step (i).\n",
    "* we will use the **TD target as our update rule for Q-values**\n",
    "* we will use the basic **L2-norm (least squares) as our loss function**\n",
    "\n",
    "#### optimizing ‚õ≥Ô∏è\n",
    "A training of ANNs requires optimization techniques to minimize a cost, e.g. to minimize some loss brought on by a function approximation. Here, we need to minimize an error from estimating Q-values so that we can get closer to final (more optimal) Q-values as we train. Therefore, we will choose an optimization procedure that will help us minimize such error by somehow updating ANN weights (Q-values) at each iteration (training step). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed14ba00685cf16edd334e781ff70fff3d3f1fc6"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tomtx/q-learning-demo/master/img/algo-bgd.png\" alt=\"Drawing\" align=\"left\" style=\"width: 210px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "57fdd8fa66b74c6ad2641896067d46374509e8de"
   },
   "source": [
    "Such optimization procedure can be done with the Batch Gradient Descent (BGD) algorithm shown with the equation above. It minimizes our loss by iteratively moving in the direction of steepest descent, which is the negative of the gradient that is calculated from the loss function. In detail, it updates our ANN weights (ùúÉ) with gradients (ùõª) of the loss function (J) at its own rate of learning (ùúÇ).\n",
    "* we will use the simple **BGD as an optimization algorithm that minimizes our loss**\n",
    "\n",
    "### Bandit üî´ ü§† üê¥\n",
    "We will create an agent that **chooses actions** as if it were a **multiarmed bandit**. It is a famous problem from probability theory, that was used to address decisions of gamblers playing slot machines in casinos. Here, it can help us to address here the exploration-exploitation dilemma in RL. This bandit will choose actions from supplied Q-values for a given state.\n",
    "* the bandit will have **4 arms**, since we have 4 actions to choose at each state\n",
    "* we will use the **epsilon-greedy** strategy for the bandit, i.e. our agent will randomly choose actions by flipping a coin so as to either go explore or exploit the environment\n",
    "* the **epsilon can take values between 0 to 1**, where 0 means an agent will do no exploration whatsoever (100% exploitation) and 1 is all about exploration (0% exploitation)\n",
    "\n",
    "## 3) Code üíª\n",
    "We will **train** the Q-learning for certain amount of episodes in the environment and then try to **play** the game with learned Q-values by training.\n",
    "\n",
    "### training üèãÔ∏è‚Äç \n",
    "> _... let's train the algorithm to solve the environment_\n",
    "\n",
    "_... load the Python **packages**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e17eaab65757e59d3ff7a5130164a6aa6ac464e9"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba7aa996e19134e6fa3b0b69f87c43daac802c15"
   },
   "source": [
    "_... set the **hyperparameters** for the model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "783ff299edf56169b5ed3f0a8a5b4d0dbc2062a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "EPISODES = 2000\n",
    "STEPS = 100\n",
    "BANDIT_ARMS = 4\n",
    "BANDIT_TRAINING = True\n",
    "EPSILON = 1 # setting the agent for exploration only -> try to learn the environment\n",
    "LEARNING_RATE = 0.2 # the learning rate for our optimization algorithm\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6d94513d16a096fa94bee088d21d82c78757f01"
   },
   "source": [
    "_... load the test environment of **frozen lake**_ and get its possible number of _**states and actions**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "de2493470d004f6a5c135012fd3202aae3e6ece1"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "states = env.observation_space.n\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed4e04bcd5d3a6519f122caf39e4674cdfeac630"
   },
   "source": [
    "_... build the **ANN** that can select actions based on the particular state from Q-values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e6346afd38a18397d9626794744b166abab6f8f"
   },
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "# -> create neurons for the input layer representing states of environemnt\n",
    "input_layer = mx.sym.Variable(name='ann_states', shape=(1, states), dtype='float32')\n",
    "\n",
    "# Weights Matrix\n",
    "# -> create a matrix with weights representing Q-values in every state of environment\n",
    "weights = mx.sym.Variable(name='ann_Q_values', shape=(states, actions), dtype='float32')\n",
    "\n",
    "# Output Layer\n",
    "# -> create neurons for the output layer representing Q-values in a particular state\n",
    "output_layer = mx.sym.dot(name='ann_Q_values_state', lhs=input_layer, rhs=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af6daa60d20ce6c43b91713acffc93fd38cecdf3"
   },
   "source": [
    "_... define the **training of ANN** to update Q-values by the agent's experience from an environment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "649d33407c289c97c5ad94653ed5c21591aa1f7b"
   },
   "outputs": [],
   "source": [
    "def train_ann(output_weights, label_weights, learning_rate):\n",
    "    # compute gradients of the loss function by differentiating a graph of NDArray operations with the chain rule\n",
    "    # -> define parameters of loss function for the following automatic differentiation\n",
    "    loss_params = mx.nd.broadcast_sub(lhs=label_weights, rhs=output_weights)\n",
    "    # -> attach gradient buffers to variables that require gradient\n",
    "    loss_params.attach_grad()\n",
    "    # -> wrap operations that need to be differentiated into a graph\n",
    "    with mx.autograd.record():\n",
    "        # add an operation for the loss function (L2)\n",
    "        diff_opp = mx.nd.sum(data=mx.nd.square(data=loss_params))\n",
    "    # -> calculate the gradients of wrapped operations with respect to their parameters\n",
    "    diff_opp.backward()\n",
    "    # -> get the gradients of the loss function\n",
    "    loss_gradients = loss_params.grad\n",
    "    \n",
    "    # update the ANN label weights via the Batch Gradient Descent (BGD) optimization algorithm\n",
    "    updated_weights = label_weights - learning_rate * loss_gradients\n",
    "    return updated_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "567ef10cc311e5d802716dd8f9ce924c04fdf573"
   },
   "source": [
    "_... define the **epsilon-greedy bandit**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48291e761365b0cdb650b7bb9fe16580146971a6"
   },
   "outputs": [],
   "source": [
    "def run_epsilon_greedy_bandit(arm_rewards, arms, training, epsilon, episode):\n",
    "    # check if bandit is used for training\n",
    "    if training:        \n",
    "        # -> start with the full exploration and slowly reduce it as this algorithm learns\n",
    "        epsilon = mx.nd.exp(data=mx.nd.array([-0.01 * episode]))\n",
    "\n",
    "    # flip the coin (randomly select the number between 0 to 1)\n",
    "    random_number = random.random()\n",
    "\n",
    "    # select the arm\n",
    "    if random_number > epsilon:\n",
    "        # Exploit\n",
    "        # -> find arms with highest rewards\n",
    "        max_reward_arms = [i for i, e in enumerate(arm_rewards) if e == max(arm_rewards)]\n",
    "        # -> select the best arm\n",
    "        if len(max_reward_arms) == 1:\n",
    "            # get the best arm\n",
    "            arm = max_reward_arms[0]\n",
    "        else:\n",
    "            # randomly choose an arm from the best arms\n",
    "            arm = random.choice(max_reward_arms)\n",
    "    else:\n",
    "        # Explore\n",
    "        # -> randomly choose an arm\n",
    "        arm = random.randrange(arms)\n",
    "        \n",
    "    return arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a277a12391b38f92993061c952d1099126599112"
   },
   "source": [
    "_... run the **Q-learning** for the test environment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "255c92040371865f531a1248326b3742e1c5f9e9"
   },
   "outputs": [],
   "source": [
    "# log the training start\n",
    "training_start = time()\n",
    "\n",
    "# store the training progress of this algorithm for each episode\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "\n",
    "# prepare the model & initialize all trainable variables before the training starts\n",
    "# -> prepare a matrix for slicing states in the input layer \n",
    "input_layer_init = mx.nd.one_hot(mx.nd.arange(16), 16)\n",
    "# -> manually initilize weights (Q-values) with random values close to zero\n",
    "weights_init = mx.nd.random_uniform(low=0, high=0.01, shape=(states, actions), dtype='float32')\n",
    "ex = weights.eval(ctx = mx.cpu(), ann_Q_values=weights_init)\n",
    "Q = ex[0]\n",
    "\n",
    "# solve the environment over certain amount of episodes\n",
    "for episode in range(EPISODES):\n",
    "    # reset the environment, rewards, and steps for the new episode\n",
    "    s = env.reset()\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    # find the solution over certain amount of attempts (steps in each episode)\n",
    "    while step < STEPS:\n",
    "        # get Q-values for the current state by feeding it forward through the ANN\n",
    "        ex = output_layer.eval(ctx = mx.cpu(), ann_states=input_layer_init[s:s+1], ann_Q_values=Q)\n",
    "        Q_current = ex[0][0]\n",
    "\n",
    "        # select the action in the current state by running the multiarmed bandit\n",
    "        a = run_epsilon_greedy_bandit(Q_current, BANDIT_ARMS, BANDIT_TRAINING, EPSILON, episode)\n",
    "\n",
    "        # enter the environment and get the experience from it by performing there an action\n",
    "        # -> get observation (new state), reward, done (success/failure), and information\n",
    "        observation, reward, done, info = env.step(a)\n",
    "\n",
    "        # get Q-values for the observed state by feeding it through the ANN \n",
    "        ex = output_layer.eval(ctx = mx.cpu(), ann_states=input_layer_init[observation:observation+1], ann_Q_values=Q)\n",
    "        Q_observed = ex[0][0]\n",
    "\n",
    "        # estimate the Q-value for the current state and action\n",
    "        # -> calculate this Q-value with the update rule using the experience from the environment\n",
    "        Q_target = Q_current.copy()\n",
    "        Q_target[a] = reward + GAMMA * mx.nd.max(Q_observed)\n",
    "\n",
    "        # train the ANN to minimize the estimation error by optimizing the Q-values\n",
    "        Q_optimized = train_ann(Q_target, Q_current, LEARNING_RATE)\n",
    "\n",
    "        # manually update the weights matrix with the trained Q-values for this state\n",
    "        Q[s] = Q_optimized\n",
    "\n",
    "        # add the reward to others during this episode\n",
    "        episode_reward += reward\n",
    "\n",
    "        # change the state to the observed state for the next iteration\n",
    "        s = observation\n",
    "\n",
    "        # check if the environment has been exited\n",
    "        if done:\n",
    "            # -> store the collected rewards & number of steps in this episode     \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_steps.append(step)\n",
    "            # -> quit the episode\n",
    "            break\n",
    "\n",
    "        # continue looping\n",
    "        step += 1\n",
    "        \n",
    "# log the training end\n",
    "training_end = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c6a815db3f63146450ef13b150f8a38257398b8"
   },
   "source": [
    "_... print the **final Q-values**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "528aeb0382d8218b3e0f41d1f059081fb56cca6e"
   },
   "outputs": [],
   "source": [
    "print(\"trained Q-values\", Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fdba99cd021496a4334005ce18b17cc635ea52ca"
   },
   "source": [
    "_... print the **training progress** of the algorithm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fa0e8aea5eac15eb20b21842db456adec9c5975"
   },
   "outputs": [],
   "source": [
    "# show the success rate for solving the environment & elapsed training time\n",
    "success_rate = round((sum(episode_rewards) / EPISODES) * 100, 2)\n",
    "elapsed_training_time = int(training_end - training_start)\n",
    "print(\"\\nThis environment has been solved\", str(success_rate), \"% of times over\",  str(EPISODES), \"episodes within\", str(elapsed_training_time), \"seconds!\")\n",
    "\n",
    "# plot the rewards and number of steps over all training episodes\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(episode_rewards, '-g', label = 'reward')\n",
    "ax1.set_yticks([0,1])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(episode_steps, '+r', label = 'step')\n",
    "ax1.set_xlabel(\"episode\")\n",
    "ax1.set_ylabel(\"reward\")\n",
    "ax2.set_ylabel(\"step\")\n",
    "ax1.legend(loc=2)\n",
    "ax2.legend(loc=1)\n",
    "plt.title(\"Training Stats\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24ffe6113a782d25afd9af1f5ad2f67b5112324d"
   },
   "source": [
    ">_... the training progress of this algorithm can be seen from the figure above, where each episode finishes either with value 0 or 1 as a reward -> this means that our agent üêπ fell into the ice hole üï≥ or found those peanuts ü•ú, respectively_\n",
    "> * the **algorithm is able to solve the environment more often as it learns** and starts exploiting its own knowledge more with increasing episodes\n",
    "> * with increasing episodes, **our agent also survives longer in the environment** without falling into the ice hole and eventually reaches the reward... how efficiently it reaches the goal is questionable though, since the ice is slippery and not being dead under it is probably important too :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2f4193689bdbb62f2b318e340effb1234b810e3"
   },
   "source": [
    "### playing üéÆ\n",
    "> _... let's play the game to see how our agent üêπ solves the environment with the knowledge obtained from training, i.e. with the trained Q-values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6b1337247d29d991b432ff7af45726cef68ef8d"
   },
   "source": [
    "_... set the **hyperparameters for the game**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eafbf54fbf4e3f9ce1934cc9aa00b573b4209714"
   },
   "outputs": [],
   "source": [
    "GAME_EPISODES = 10\n",
    "BANDIT_TRAINING = False\n",
    "EPSILON = 0 # setting the agent for pure exploitation -> use only learned Q values from training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6caae265bb96f4487292b98d1917a89e084075a"
   },
   "source": [
    "_... let's **play the game**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b8bc02ccef6634d5ee6389e8ee79baf7fd5517b"
   },
   "outputs": [],
   "source": [
    "for episode in range(GAME_EPISODES):\n",
    "    s = env.reset()\n",
    "    step = 0\n",
    "    while step < STEPS:\n",
    "        # take the action that have the maximum expected future reward\n",
    "        a = run_epsilon_greedy_bandit(Q[s, :], BANDIT_ARMS, BANDIT_TRAINING, EPSILON, episode)\n",
    "        \n",
    "        # enter the environment and get the experience from it\n",
    "        observation, reward, done, info = env.step(a)\n",
    "\n",
    "        # change the state to observed state for the next iteration\n",
    "        s = observation\n",
    "\n",
    "        # check if the environment has been exited\n",
    "        if done:\n",
    "            # print results of each episode\n",
    "            print(\"\\n-----------------------------\")\n",
    "            print(step, \"steps in episode\", episode)\n",
    "            print(\"The last action & state was:\")\n",
    "            env.render()\n",
    "            print(\"-----------------------------\")\n",
    "            # quit the episode\n",
    "            break\n",
    "\n",
    "        # continue looping\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10afca93e0b877bf0ff451f9b4cd030f62500308"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tomtx/q-learning-demo/master/img/conclusions-wall-e.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d95a35f0b10fa7ad56da03d46aa66d56f410c11"
   },
   "source": [
    "## Conclusions ü§î\n",
    "We have built a simple neural net model for Q-learning solving an RL problem of a frozen lake. As seen from running the code, our model learns during training and then takes advantage of such prior experience when solving the environment during playing. We have used a low learning rate for the optimization algorithm in order for our solution to be more precise, since we do not care much about the time it will cost us due to simplicity of this model. Also, here we have perhaps used an overly simple loss function that is not robust enough to deal with noisy Q estimates. However, you can tweak the hyperparameters for discount factor, choose different loss functions, or even implement different bandit strategies for choosing actions to see if it learns better during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "010510cceb0fae85a725cfe48334c3458c15b24a"
   },
   "source": [
    "## References\n",
    "* R. S. Sutton and A. G. Barto: \"Reinforcement Learning: An Introduction\", 2nd edition, A Bradford Book, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
